{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8071e1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e72bdf79a1496798375a11a70cbf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "cache_root = \"/home/tim/xai-hackathon-12-06-25/assets/huggingface\"\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"   # same as above\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_root, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=cache_root,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94f61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attention mechanisms allow neural networks to focus on specific parts of input data when making predictions or decisions, improving performance in tasks such as natural language processing and image recognition.\n",
      "logprobs: [-0.014956246130168438, -5.376194530981593e-05, -0.5838623046875, -0.7278666496276855, -0.0004903068183921278, -4.768360213347478e-06, -0.009762637317180634, -1.3708974620385561e-05, -0.5793320536613464, -0.0046726795844733715, -1.1920928244535389e-07, -0.04663544520735741, -0.000303818320389837, -0.06301615387201309, -0.24273931980133057, -0.018959958106279373, -0.4965890049934387, -0.01852274499833584, -0.5918033123016357, -0.03130823001265526, -0.32091835141181946, -0.15038171410560608, -0.0969526395201683, -0.4741094708442688, 0.0, -0.925885796546936, -3.814689989667386e-06, -7.676783570786938e-05, -0.00016306500765495002, -0.1457817703485489, -0.12014768272638321, -0.00011169286881340668, -0.08405431360006332]\n",
      "total logprob: -5.749480303435568\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write one sentence about attention mechanisms.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "seq = gen.sequences[0]\n",
    "gen_ids = seq[prompt_len:]\n",
    "scores = gen.scores  # list of logits, one per generated token\n",
    "\n",
    "logprobs = []\n",
    "for i, tid in enumerate(gen_ids):\n",
    "    lp = torch.log_softmax(scores[i][0], dim=-1)[tid].item()\n",
    "    logprobs.append(lp)\n",
    "\n",
    "text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "print(text)\n",
    "print(\"logprobs:\", logprobs)\n",
    "print(\"total logprob:\", sum(logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b482eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file = 'assets/data/MATH500.json'\n",
    "dataset = json.load(open(json_file, \"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from utils.parse_utils import parse_answer\n",
    "from constants import *\n",
    "from power_samp_utils import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--save_str\", action = \"store\", type = str, default = \"results/\",  dest = \"save_str\")\n",
    "    parser.add_argument(\"--model\", action = \"store\", default = \"qwen\", type = str, choices = [\"qwen\", \"qwen_math\", \"phi\", \"tulu\", \"qwen_math_grpo\", \"phi_grpo\"])\n",
    "    parser.add_argument(\"--temperature\", action = \"store\", default = 0.25, type = float, dest = \"temperature\")\n",
    "    parser.add_argument(\"--dataset\", action = \"store\", default = \"MATH\", type = str)\n",
    "    parser.add_argument(\"--cot\", action = \"store\", type = bool, default = True)\n",
    "    parser.add_argument(\"--mcmc_steps\", action = \"store\", type = int, default = 10)\n",
    "    parser.add_argument(\"--device\", action = \"store\", type = str, dest = \"device\", default = \"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    parser.add_argument(\"--batch_idx\", action = \"store\", type = int, default = 0)\n",
    "    parser.add_argument(\"--seed\", action = \"store\", type = int, default = 0)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "\n",
    "    model = args.model\n",
    "    device = args.device\n",
    "    dataset_name = args.dataset\n",
    "    cot = args.cot\n",
    "    temp = args.temperature\n",
    "    mcmc_steps = args.mcmc_steps\n",
    "\n",
    "    save_str = os.path.join(args.save_str, model)\n",
    "    os.makedirs(save_str, exist_ok=True)\n",
    "\n",
    "\n",
    "    print(model)\n",
    "    print(device)\n",
    "    print(mcmc_steps)\n",
    "    if model == \"qwen\":\n",
    "        model_str = \"Qwen/Qwen2.5-7B\"\n",
    "    elif model == \"qwen_math\":\n",
    "        model_str = \"Qwen/Qwen2.5-Math-7B\"\n",
    "    elif model == \"qwen_math_grpo\":\n",
    "        model_str = \"stellalisy/rethink_rlvr_reproduce-ground_truth-qwen2.5_math_7b-lr5e-7-kl0.00-step150\"\n",
    "    elif model == \"phi\":\n",
    "        model_str = 'microsoft/Phi-3.5-mini-instruct'\n",
    "    elif model == \"tulu\":\n",
    "        model_str = \"allenai/Llama-3.1-Tulu-3-8B-DPO\"\n",
    "\n",
    "    if dataset_name == \"MATH\":\n",
    "        json_file = 'data/MATH500.json'\n",
    "        dataset = json.load(open(json_file, \"r\"))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"dataset done\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_str, trust_remote_code = True)\n",
    "    hf_model = transformers.AutoModelForCausalLM.from_pretrained(model_str, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code = True).to(device)\n",
    "    autoreg_sampler = AutoregressiveSampler(hf_model, tokenizer, device)\n",
    "\n",
    "    print(\"loaded models\")\n",
    "    results = []\n",
    "\n",
    "    start = 100*args.batch_idx\n",
    "    end = 100*(args.batch_idx+1)\n",
    "\n",
    "    for problem, data in tqdm(enumerate(dataset[start:end]), desc = \"Benchmark on MATH\"):\n",
    "        question = data[\"prompt\"]\n",
    "        print(question)\n",
    "        answer = data[\"answer\"]\n",
    "\n",
    "        input_text = format_prompt(question, model, tokenizer, cot)\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "        prefx = [idx.item() for idx in input_ids[0]]\n",
    "\n",
    "        naive_temp_output = hf_model.generate(input_ids, max_new_tokens=3072, \n",
    "                                return_dict_in_generate=True, output_scores=True, do_sample = True, temperature = temp)\n",
    "        \n",
    "        print(tokenizer.decode(naive_temp_output[0][:, len(input_ids[0]):].squeeze().to(\"cpu\"), skip_special_tokens=True))\n",
    "        print(\"naive done\")\n",
    "        \n",
    "        \n",
    "        std_output = hf_model.generate(input_ids, max_new_tokens=3072, \n",
    "                                return_dict_in_generate=True, output_scores=True, do_sample = True)\n",
    "        \n",
    "        print(tokenizer.decode(std_output[0][:, len(input_ids[0]):].squeeze().to(\"cpu\"), skip_special_tokens=True))\n",
    "        print(\"std done\")\n",
    "\n",
    "        mcmc_power_samp_output, _, _, acceptance_ratio = mcmc_power_samp(autoreg_sampler, prefx, temp, mcmc_steps, max_new_tokens=3072)\n",
    "\n",
    "        print(len(std_output))\n",
    "        print(len(naive_temp_output))\n",
    "        print(len(mcmc_power_samp_output))\n",
    "        print(tokenizer.decode(torch.tensor([mcmc_power_samp_output], dtype=torch.long, device=device).squeeze().to(\"cpu\"), skip_special_tokens=True))\n",
    "        print(\"mcmc done\")\n",
    "\n",
    "        naive_generated_ids = naive_temp_output[0][:, len(input_ids[0]):].squeeze().to(\"cpu\")\n",
    "        std_generated_ids = std_output[0][:, len(input_ids[0]):].squeeze().to(\"cpu\")\n",
    "        mcmc_power_samp_ids = torch.tensor([mcmc_power_samp_output], dtype=torch.long, device=device).squeeze().to(\"cpu\")\n",
    "\n",
    "        naive_completion = tokenizer.decode(naive_generated_ids, skip_special_tokens=True)\n",
    "        std_completion = tokenizer.decode(std_generated_ids, skip_special_tokens=True)\n",
    "        mcmc_completion = tokenizer.decode(mcmc_power_samp_ids, skip_special_tokens=True)\n",
    "\n",
    "        naive_answer = parse_answer(naive_completion)\n",
    "        std_answer = parse_answer(std_completion)\n",
    "        mcmc_answer = parse_answer(mcmc_completion)\n",
    "        \n",
    "        print(naive_answer)\n",
    "        print(std_answer)\n",
    "        print(mcmc_answer)\n",
    "        print(question)\n",
    "        print(answer)\n",
    "        print(f'Acceptance: {acceptance_ratio}')\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"correct_answer\": answer,\n",
    "            \"naive_completion\": naive_completion,\n",
    "            \"naive_answer\": naive_answer,\n",
    "            \"std_completion\": std_completion,\n",
    "            \"std_answer\": std_answer,\n",
    "            \"mcmc_completion\": mcmc_completion,\n",
    "            \"mcmc_answer\": mcmc_answer,\n",
    "        })\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(save_str, model+\"_math_base_power_samp_results_\" + str(mcmc_steps) + \"_\" + str(temp) + \"_\" + str(args.batch_idx)  + \"_\" + str(args.seed) + \".csv\"), index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
