{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8071e1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259d51ae76b84df1a41de1e96b6e1a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "cache_root = \"/home/tim/xai-hackathon-12-06-25/assets/huggingface\"\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"   # same as above\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_root, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=cache_root,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94f61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attention mechanisms allow neural networks to focus on specific parts of input data when making predictions or decisions, improving performance in tasks such as natural language processing and image recognition.\n",
      "logprobs: [-0.014956246130168438, -5.376194530981593e-05, -0.5838623046875, -0.7278666496276855, -0.0004903068183921278, -4.768360213347478e-06, -0.009762637317180634, -1.3708974620385561e-05, -0.5793320536613464, -0.0046726795844733715, -1.1920928244535389e-07, -0.04663544520735741, -0.000303818320389837, -0.06301615387201309, -0.24273931980133057, -0.018959958106279373, -0.4965890049934387, -0.01852274499833584, -0.5918033123016357, -0.03130823001265526, -0.32091835141181946, -0.15038171410560608, -0.0969526395201683, -0.4741094708442688, 0.0, -0.925885796546936, -3.814689989667386e-06, -7.676783570786938e-05, -0.00016306500765495002, -0.1457817703485489, -0.12014768272638321, -0.00011169286881340668, -0.08405431360006332]\n",
      "total logprob: -5.749480303435568\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write one sentence about attention mechanisms.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "seq = gen.sequences[0]\n",
    "gen_ids = seq[prompt_len:]\n",
    "scores = gen.scores  # list of logits, one per generated token\n",
    "\n",
    "logprobs = []\n",
    "for i, tid in enumerate(gen_ids):\n",
    "    lp = torch.log_softmax(scores[i][0], dim=-1)[tid].item()\n",
    "    logprobs.append(lp)\n",
    "\n",
    "text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "print(text)\n",
    "print(\"logprobs:\", logprobs)\n",
    "print(\"total logprob:\", sum(logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c5bea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_answer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpower_samp_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_prompt, AutoregressiveSampler, mcmc_power_samp\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m model_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/home/tim/xai-hackathon-12-06-25/src/utils/power_samp_utils.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m### DESCRIPTION ###\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# power sampling to sample from p^{alpha}, where p is the base model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# takes in 1/alpha (temperature) as an argument (default 0.25), and mcmc_power_samp implements sampling from p^{alpha} \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAutoregressiveSampler\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "import json, random, os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "from utils.parse_utils import parse_answer\n",
    "from constants import *\n",
    "from utils.power_samp_utils import format_prompt, AutoregressiveSampler, mcmc_power_samp\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_str = \"Qwen/Qwen2.5-7B\"\n",
    "json_file = \"data/MATH500.json\"\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_str, trust_remote_code=True)\n",
    "hf_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_str, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "autoreg_sampler = AutoregressiveSampler(hf_model, tokenizer, device)\n",
    "\n",
    "def run_qwen(dataset, start=0, end=100, cot=True, temp=0.25, mcmc_steps=10, max_new_tokens=3072, seed=0):\n",
    "    random.seed(seed)\n",
    "    results = []\n",
    "    for _, data in tqdm(list(enumerate(dataset[start:end])), desc=\"MATH\"):\n",
    "        question = data[\"prompt\"]\n",
    "        answer = data[\"answer\"]\n",
    "\n",
    "        input_text = format_prompt(question, \"qwen\", tokenizer, cot)\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "        prefx = input_ids[0].tolist()\n",
    "\n",
    "        naive_temp_out = hf_model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "        )\n",
    "        std_out = hf_model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        mcmc_ids, _, _, acc = mcmc_power_samp(\n",
    "            autoreg_sampler, prefx, temp, mcmc_steps, max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "        naive_ids = naive_temp_out[0][:, len(prefx):].squeeze().to(\"cpu\")\n",
    "        std_ids = std_out[0][:, len(prefx):].squeeze().to(\"cpu\")\n",
    "        mcmc_ids_t = torch.tensor(mcmc_ids, dtype=torch.long).to(\"cpu\")\n",
    "\n",
    "        naive_completion = tokenizer.decode(naive_ids, skip_special_tokens=True)\n",
    "        std_completion = tokenizer.decode(std_ids, skip_special_tokens=True)\n",
    "        mcmc_completion = tokenizer.decode(mcmc_ids_t, skip_special_tokens=True)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"correct_answer\": answer,\n",
    "            \"naive_completion\": naive_completion,\n",
    "            \"naive_answer\": parse_answer(naive_completion),\n",
    "            \"std_completion\": std_completion,\n",
    "            \"std_answer\": parse_answer(std_completion),\n",
    "            \"mcmc_completion\": mcmc_completion,\n",
    "            \"mcmc_answer\": parse_answer(mcmc_completion),\n",
    "            \"acceptance_ratio\": acc,\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# df = run_qwen(dataset, start=0, end=100, cot=True, temp=0.25, mcmc_steps=10)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e76a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
