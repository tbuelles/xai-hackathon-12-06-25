{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8071e1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259d51ae76b84df1a41de1e96b6e1a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "cache_root = \"/home/tim/xai-hackathon-12-06-25/assets/huggingface\"\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"   # same as above\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_root, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=cache_root,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94f61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attention mechanisms allow neural networks to focus on specific parts of input data when making predictions or decisions, improving performance in tasks such as natural language processing and image recognition.\n",
      "logprobs: [-0.014956246130168438, -5.376194530981593e-05, -0.5838623046875, -0.7278666496276855, -0.0004903068183921278, -4.768360213347478e-06, -0.009762637317180634, -1.3708974620385561e-05, -0.5793320536613464, -0.0046726795844733715, -1.1920928244535389e-07, -0.04663544520735741, -0.000303818320389837, -0.06301615387201309, -0.24273931980133057, -0.018959958106279373, -0.4965890049934387, -0.01852274499833584, -0.5918033123016357, -0.03130823001265526, -0.32091835141181946, -0.15038171410560608, -0.0969526395201683, -0.4741094708442688, 0.0, -0.925885796546936, -3.814689989667386e-06, -7.676783570786938e-05, -0.00016306500765495002, -0.1457817703485489, -0.12014768272638321, -0.00011169286881340668, -0.08405431360006332]\n",
      "total logprob: -5.749480303435568\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write one sentence about attention mechanisms.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "seq = gen.sequences[0]\n",
    "gen_ids = seq[prompt_len:]\n",
    "scores = gen.scores  # list of logits, one per generated token\n",
    "\n",
    "logprobs = []\n",
    "for i, tid in enumerate(gen_ids):\n",
    "    lp = torch.log_softmax(scores[i][0], dim=-1)[tid].item()\n",
    "    logprobs.append(lp)\n",
    "\n",
    "text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "print(text)\n",
    "print(\"logprobs:\", logprobs)\n",
    "print(\"total logprob:\", sum(logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bece91268d4399aadf6c9da22fbab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MATH:   0%|          | 0/1 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 4.0\n",
      "3072\n",
      "192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "  6%|▋         | 1/16 [00:06<01:41,  6.74s/it]\n",
      "MATH: 100%|██████████| 1/1 [00:20<00:00, 20.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>naive_completion</th>\n",
       "      <th>naive_answer</th>\n",
       "      <th>std_completion</th>\n",
       "      <th>std_answer</th>\n",
       "      <th>mcmc_completion</th>\n",
       "      <th>mcmc_answer</th>\n",
       "      <th>acceptance_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Convert the point $(0,3)$ in rectangular coord...</td>\n",
       "      <td>\\left( 3, \\frac{\\pi}{2} \\right)</td>\n",
       "      <td>To convert the point $(0,3)$ from rectangular...</td>\n",
       "      <td>None</td>\n",
       "      <td>As given, we have a rectangular coordinate $(...</td>\n",
       "      <td>None</td>\n",
       "      <td>Can you solve the following math problem? Conv...</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Convert the point $(0,3)$ in rectangular coord...   \n",
       "\n",
       "                    correct_answer  \\\n",
       "0  \\left( 3, \\frac{\\pi}{2} \\right)   \n",
       "\n",
       "                                    naive_completion naive_answer  \\\n",
       "0   To convert the point $(0,3)$ from rectangular...         None   \n",
       "\n",
       "                                      std_completion std_answer  \\\n",
       "0   As given, we have a rectangular coordinate $(...       None   \n",
       "\n",
       "                                     mcmc_completion mcmc_answer  \\\n",
       "0  Can you solve the following math problem? Conv...          {}   \n",
       "\n",
       "   acceptance_ratio  \n",
       "0               0.5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, random, os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "from parse_utils import parse_answer\n",
    "from constants import *\n",
    "from power_samp_utils import format_prompt, AutoregressiveSampler, mcmc_power_samp\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_str = \"Qwen/Qwen2.5-7B\"\n",
    "json_file = \"../assets/data/MATH500.json\"\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_str, trust_remote_code=True)\n",
    "hf_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_str, dtype=\"auto\", device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "autoreg_sampler = AutoregressiveSampler(hf_model, tokenizer, device)\n",
    "\n",
    "def run_qwen(dataset, start=0, end=100, cot=True, temp=0.25, mcmc_steps=10, max_new_tokens=3072, seed=0):\n",
    "    random.seed(seed)\n",
    "    results = []\n",
    "    for _, data in tqdm(list(enumerate(dataset[start:end])), desc=\"MATH\"):\n",
    "        question = data[\"prompt\"]\n",
    "        answer = data[\"answer\"]\n",
    "\n",
    "        input_text = format_prompt(question, \"qwen\", tokenizer, cot)\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "        prefx = input_ids[0].tolist()\n",
    "\n",
    "        naive_temp_out = hf_model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "        )\n",
    "        std_out = hf_model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        mcmc_ids, _, _, acc = mcmc_power_samp(\n",
    "            autoreg_sampler, prefx, temp, mcmc_steps, max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "        naive_ids = naive_temp_out[0][:, len(prefx):].squeeze().to(\"cpu\")\n",
    "        std_ids = std_out[0][:, len(prefx):].squeeze().to(\"cpu\")\n",
    "        mcmc_ids_t = torch.tensor(mcmc_ids, dtype=torch.long).to(\"cpu\")\n",
    "\n",
    "        naive_completion = tokenizer.decode(naive_ids, skip_special_tokens=True)\n",
    "        std_completion = tokenizer.decode(std_ids, skip_special_tokens=True)\n",
    "        mcmc_completion = tokenizer.decode(mcmc_ids_t, skip_special_tokens=True)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"correct_answer\": answer,\n",
    "            \"naive_completion\": naive_completion,\n",
    "            \"naive_answer\": parse_answer(naive_completion),\n",
    "            \"std_completion\": std_completion,\n",
    "            \"std_answer\": parse_answer(std_completion),\n",
    "            \"mcmc_completion\": mcmc_completion,\n",
    "            \"mcmc_answer\": parse_answer(mcmc_completion),\n",
    "            \"acceptance_ratio\": acc,\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df = run_qwen(dataset, start=0, end=1, cot=True, temp=0.25, mcmc_steps=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf59a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
